# Coding-Three-Exploring-to-Machine-Intelligence
Project video link:  
https://youtu.be/IZmF7LmnKsE  
Overview:  
This project attempts to apply different artistic styles of images to the same input image by using Neural Style Transfer and Pytorch Style Transfer. The diversity, Style expression and creativity of the images generated by Neural Style Transfer and Pytorch Style Transfer are observed by adjusting the relevant data such as optimizer and parameters.
Note: Originally I wanted to study the difference between Neural Style Transfer and CycleGan. However, it is later found that because CycleGan needs a large number of data sets to train, it is really a little different from Neural Style Transfer, which cannot achieve the effect of "applying images with different artistic styles to the same input image". Because CycleGan needs a large number of data sets in both content image and learning image, the generated effect is inevitably different from Neural Style Transfer. On this level, I later gave up comparing Neural Style Transfer with CycleGan. (Even though I have looked for a lot of materials in the early stage, I think it is not rigorous enough to make an effective comparison.) Both Neural Style Transfer and Pytorch Style Transfer can achieve the effect of "applying images with different artistic styles to the same input image". So I finally chose these two as the objects of observation.  
Dataset reference:  
https://www.kaggle.com/datasets/bryanb/abstract-art-gallery  
https://www.kaggle.com/datasets/robgonsalves/impressionistlandscapespaintings  
Own integrated data sets:  
https://drive.google.com/drive/folders/1L-a1FRpIhlULrBTXF7CnewFV3fBRo9tP?usp=sharing  
My Google Drive：  
https://drive.google.com/drive/folders/1UU6WNxIQERTJRdRkXgNVg83LkBLwXrme?usp=drive_link  
# Neural Style Transfer  
Reference:  
https://github.com/smara97/Machine-Learning-Projects/blob/master/Neural%20Style%20Transfer/neural-style-transfer.ipynb  

## Version 1  
Code link:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/blob/main/code-Neural%20Style%20Transfer%20Version%201.ipynb  
Modified content:  
1 Added the section connecting to Google.colab.  
2 Using chatgpt helped me change the code below in the reference to something that works. (Because the code in the reference doesn't work, I tried many times without success, and finally changed the code according to chatgpt.)  
Refer to the original unworkable code paragraph:  
```
content_features=get_features(content,vgg)     #Extract Features 

style_features=get_features(style,vgg)

style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}

target = content.clone().requires_grad_(True).to(“cuda")
``` 
After asking chatgpt, follow the prompts and help after changing the code:  
``` 
content = content.cuda()  
style = style.cuda()  

content_features = get_features(content, vgg)     # Extract Features
style_features = get_features(style, vgg)

style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}

target = content.clone().requires_grad_(True)  
``` 
3 The image can be extracted from the google.colab folder.  
Effect picture:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/blob/main/Neural%20Style%20Transfer%20Version%201/Neural%20Style%20Transfer%20Version%201-Effect%20picture.pdf  
Process picture:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/blob/main/Neural%20Style%20Transfer%20Version%201/Neural%20Style%20Transfer%20Version%201-Process%20picture.pdf  
All tried codes:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/tree/main/Neural%20Style%20Transfer%20Version%201/Procedure%20code  
Collection of output images of all attempts:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/tree/main/Neural%20Style%20Transfer%20Version%201/Output%20picture  
Reflection and summary:  
In this attempt, the final output of the image effect is more like the style of the image, of course, but also retained a lot of content image content. It is not difficult to see a gradual change in style in the process. But I think the style change is slower, and I want the output image to be a little more like the style image. But overall it's a good start. At least it works. In fact, I tried other references before this one but the results were not very good, so I was satisfied that I could succeed the first time.  

## Version 2
Code link:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/blob/main/code-Neural%20Style%20Transfer%20Version%202.ipynb  
Modified content:  
1 Increases the value of content_weight  
2 Reduced the value of style_weight  
3 increases the value of lr  
Effect picture:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/blob/main/Neural%20Style%20Transfer%20Version%202/Neural%20Style%20Transfer%20Version%202-Effect%20picture.pdf  
Process picture:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/blob/main/Neural%20Style%20Transfer%20Version%202/Neural%20Style%20Transfer%20Version%202-Process%20picture.pdf  
All tried codes:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/tree/main/Neural%20Style%20Transfer%20Version%202/Procedure%20code  
Collection of output images of all attempts:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/tree/main/Neural%20Style%20Transfer%20Version%202/Output%20picture  
Reflection and summary:  
In this version, I tried to keep the content-image ratio a little more. This time, only the first photo is strong, and better than the previous version, but most of the results are not very good. Only part of the orange has shifted to style. Although it is true that the proportion of content in the picture has increased, the resulting picture is very strange. As can be seen from the change process diagram, the change is relatively small. But I have looked at it carefully, and I find that there is a change in the level of detail. Overall, it will behave a little more like an orange slice than the previous version. But some orange slices are weird and some are blue, which is not good.  
## Version 3
Code link:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/blob/main/code-Neural%20Style%20Transfer%20Version%203.ipynb  
Modified content:  
1 I tried to reduce the value of content_weight (but found that the reduction was not very good, so I chose to increase the value of style_weight and leave the value of content_weight unchanged)  
2 Added the value of style_weight  
3. Increased the value of lr (modified the value of lr many times because I hoped that the output picture would be closer to the style picture with a higher learning rate)  
4 Try to replace the optimizer Adam, chatgpt told me to try SGD and LBFGS. So I made an attempt on this basis. First, I replaced the following code. Here, I also asked chatgpt to help me check whether the modified code was correct.  
The original:  
``` 
show_every = 1000  

# iteration hyperparameters
optimizer = optim.Adam([target], lr=0.1)
steps = 5000  # decide how many iterations to update your image (5000)

for ii in range(1, steps+1):

    # get the features from your target image
    target_features = get_features(target, vgg)

    # the content loss
    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)

    # the style loss
    # initialize the style loss to 0
    style_loss = 0
    # then add to it for each layer's gram matrix loss
    for layer in style_weights:
        # get the "target" style representation for the layer
        target_feature = target_features[layer]
        target_gram = gram_matrix(target_feature)
        _, d, h, w = target_feature.shape
        # get the "style" style representation
        style_gram = style_grams[layer]
        # the style loss for one layer, weighted appropriately
        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)
        # add to the style loss
        style_loss += layer_style_loss / (d * h * w)

    # calculate the *total* loss
    total_loss = content_weight * content_loss + style_weight * style_loss

    # update your target image
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    # display intermediate images and print the loss
    if  ii % show_every == 0:
        print('Total loss: ', total_loss.item())
        plt.imshow(im_convert(target))
        plt.show()
``` 
The revised:
``` 
show_every = 2000

# Iteration hyperparameters
optimizer = torch.optim.LBFGS([target], lr=0.005)
steps = 10000  # Decide how many iterations to update your image (5000)

def closure():
    # Get the features from your target image
    target_features = get_features(target, vgg)

    # The content loss
    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)

    # The style loss
    # Initialize the style loss to 0
    style_loss = 0
    # Then add to it for each layer's gram matrix loss
    for layer in style_weights:
        # Get the "target" style representation for the layer
        target_feature = target_features[layer]
        target_gram = gram_matrix(target_feature)
        _, d, h, w = target_feature.shape
        # Get the "style" style representation
        style_gram = style_grams[layer]
        # The style loss for one layer, weighted appropriately
        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)
        # Add to the style loss
        style_loss += layer_style_loss / (d * h * w)

    # Calculate the *total* loss
    total_loss = content_weight * content_loss + style_weight * style_loss

    # Update your target image
    optimizer.zero_grad()
    total_loss.backward()

    return total_loss

# Optimization loop
for ii in range(1, steps+1):
    optimizer.step(closure)

    # Display intermediate images and print the loss
    if ii % show_every == 0:
        print('Total loss: ', closure().item())
        plt.imshow(im_convert(target))
        plt.show()
``` 
But unfortunately, the output result after modification is not very good.  
First of all, in terms of running, the optimizer change uses a lot of Gpus, the running time becomes very long, but after several attempts, the output is black, I also asked chatgpt about this. It tells me that there are four possible causes.  
1 Problems in the training process: There may be problems in the training process, resulting in the generated image does not correctly capture the relationship between content and style. This may be due to improper selection of hyperparameters or insufficient number of training iterations. Increasing the number of training iterations or adjusting hyperparameters, such as content weights and style weights, can try to improve results.  
2 Fade out: If the fade disappears during the process of generating the correct image, the image may not be updated. This may be due to gradients becoming too severely small in some layers of the network to produce effective updates. You can try to solve this problem by using a pre-trained model or adjusting the network architecture.  
3 Data range problem: Neural networks are typically trained using data that ranges from 0 to 1. If the pixel value range of the image is not handled correctly during the training process, the generated image may have problems. Ensure proper normalization and de-normalization of images during training and generation.  
4 Image saving problems: Problems may occur when saving the generated images. Make sure to use the correct file format and parameters when saving images.
But I tried many times and still couldn't solve it, so I tried changing the values of show_every and steps to show_every = 10, steps = 50. Here are the results of the attempt.  
LBFGS：  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/tree/main/LBFGS  
SGD：  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/tree/main/SGD  
As you can see from the pictures, the effect is not very good, and they use too much gpu, I think it is not as good as Adam, so I chose Adam.  
Effect picture:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/blob/main/Neural%20Style%20Transfer%20Version%203/Neural%20Style%20Transfer%20Version%203-Effect%20picture.pdf  
Process picture:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/blob/main/Neural%20Style%20Transfer%20Version%203/Neural%20Style%20Transfer%20Version%203-Process%20picture.pdf  
All tried codes:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/tree/main/Neural%20Style%20Transfer%20Version%203/Procedure%20code  
Collection of output images of all attempts:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/tree/main/Neural%20Style%20Transfer%20Version%203/Output%20picture  
Reflection and summary:  
I prefer the images produced in this version. Because my initial idea was that I wanted the output image to be more like a style image. In this version, I think the output picture not only retains the content of the original picture, but also looks more like the style picture, with a very strong style! This is more in line with my expectations. From the process picture, the first output picture and the last output picture is still changed. The final output of the image will be more intense and detailed than the first output of the image. I think this is a good version, more in line with my expectations.  


# Pytorch Style Transfer
Reference:   
https://github.com/jeremycochoy/style-transfer/tree/master  

## Version 1
Code link:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/blob/main/Pytorch%20Style%20Transfer%20Version%201.ipynb  
Modified content:  
1 Change the original reference to "find images directly from the Web" to extract them from google Drive. Some of the code has changed in this section.  
2 Slight modifications to the original code. Remove some unnecessary code.  
3 In order to compare with the image generated by Neural Style Transfer. I changed the values of IMAGE_WIDTH and IMAGE_HEIGHT. Make the final output image the same size as the one generated by Neural Style Transfer.  
Effect picture:  
https://drive.google.com/file/d/1JhlJSN-_jaGYOI-_oO93LqkH0UlZBgfu/view?usp=drive_link  
Process picture:  
https://drive.google.com/file/d/1iCBKEr2oqGFVlK50vy-kHXgMtaSniqxC/view?usp=drive_link  
All tried codes:  
https://drive.google.com/drive/folders/1EreUDao0x1YFdvg78iSVgAlxPDhv-fpq?usp=drive_link  
Collection of output images of all attempts:  
https://drive.google.com/drive/folders/1ZILFaxD7TjOLU8nrgUFglZmc9V3jAOEc?usp=drive_link  
Reflection and summary:  
The first attempt I think gave me quite a few surprises. The image output by Pytorch Style Transfer seems to be more in line with my aesthetic than the image output by Neural Style Transfer. I think Pytorch Style Transfer can better learn the style of style pictures. The output image has a very strong style. That's a good start! Gave me the confidence to keep training it.  

## Version 2
Code link:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/blob/main/Pytorch%20Style%20Transfer%20Version%202.ipynb  
Modified content:  
1 In this attempt, I want the output image to be a little more like a style image. So I adjusted the values of CONTENT_WEIGHT and STYLE_WEIGHT. Of course, it wasn't very successful at first. Maybe I was too eager, resulting in the final output of the image is too much like the style image, the proportion of the content image is too small. After many attempts, I settled on CONTENT_WEIGHT = 0.7. Also, STYLE_WEIGHT = 0.03.  
2 Because I want the generated image to have more detail and texture, I have reduced the value of TOTAL_VARIATION_WEIGHT a bit.  
3 I wanted to try different layers to calculate content loss and style loss, so I changed the code below.  
The original:  
``` 
content_layers = ['conv_1', 'conv_2', 'conv_4']
style_layers = ['conv_2', 'conv_3', 'conv_4', 'conv_7', 'conv_10', ‘conv_8']
``` 
After the change:
``` 
content_layers = ['conv_3', 'conv_5', 'conv_7']
style_layers = ['conv_1', 'conv_4', 'conv_6', 'conv_9', ‘conv_11']
``` 
4 Change ITERATIONS = 20 to ITERATIONS = 10 (because I think producing 20 process pictures is too much, I don't need so many process pictures)  
5 Changed the number of forwarding times. The original 2000 has been increased to 3000. It is hoped that the model will have more opportunities for optimization and adjustment to further approximate the target style.  
6 Change INPUT_IMAGE ='noise' to INPUT_IMAGE ='style'  
Effect picture:  
https://drive.google.com/file/d/1quiK95hERD0PnmWAD_EHNdYBb-6eIagh/view?usp=drive_link  
Process picture:  
https://drive.google.com/file/d/13VjKT56ggx5k9ZzbxOzIA8SK2P3PGquC/view?usp=drive_link  
All tried codes:  
https://drive.google.com/drive/folders/1pj_foEwkpZeCHe5LF0vICjhXdkwg3U02?usp=drive_link  
Collection of output images of all attempts:  
https://drive.google.com/drive/folders/1j8cco_ecmmemTYoWFM9XonYesbZWiaI5?usp=drive_link  
Reflection and summary:  
In this version, first of all, the texture is really much better than the previous version. The overall picture texture is much stronger, and the feeling will be more concave and convex. This is more in line with my expectations.  
In this version, it may be because it is closer to the style picture. So from the process picture it feels more like the "orange slice" picture is slowly superimposed on top of the style picture. (It may be modified to become a style picture with the texture of 'orange slice')  
This version will be more like "orange slices" superimposed on top of the style picture. But it's also a good version and one I'm happy with, and I think the output is quite interesting.  

## Version 3
Code link:  
https://github.com/lingyizhang-ual/Coding-Three-Exploring-to-Machine-Intelligence/blob/main/code-Neural%20Style%20Transfer%20Version%203.ipynb  
Modified content:  
1 In the last attempt, the proportion of style pictures seems to be more. In this version, I want the content to be more graphic. So I changed the STYLE_WEIGHT to 0.05.  
2 Change INPUT_IMAGE = 'style' to INPUT_IMAGE = 'content'.  
3. In order to keep more "style and texture" of style pictures, I choose to reduce TOTAL_VARIATION_WEIGHT and set TOTAL_VARIATION_WEIGHT to 10. This will make the output image more textured.  
4 I wanted to try different layers to calculate content loss and style loss, so I changed the code below.  
The original:  
``` 
content_layers = ['conv_1', 'conv_2', 'conv_4']
style_layers = ['conv_2', 'conv_3', 'conv_4', 'conv_7', 'conv_10', ‘conv_8’]
``` 
After the change:
``` 
content_layers = ['conv_1', 'conv_2', ‘conv_4']
style_layers = ['relu_2', 'relu_3', 'relu_4', 'relu_7', 'relu_10', 'relu_8']
``` 
(With note) : I originally wanted to change it to "content_layers = ['relu3_1', 'relu4_1', 'relu5_1']
Style_layers = [' relu1_1 ', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1] ". But the code keeps reporting errors. I tried many times and asked chatgpt. But it still didn't work out very well. The final output image is also gray. Finally I had to give up the idea.  
5 I changed the optimizer LBFGS to the optimizer Adam.  
Effect picture:  
https://drive.google.com/file/d/1olp0xUVJlAPWT3DM2x07Tq5StB0dScHn/view?usp=drive_link  
Process picture:  
https://drive.google.com/file/d/1dmkaSlbglw4TQ34oa2GdoFyYn-r6m-o4/view?usp=drive_link  
All tried codes:  
https://drive.google.com/drive/folders/19NAktXTiuCTbfEDVkv2jlChMXVExmomi?usp=drive_link  
Collection of output images of all attempts:  
https://drive.google.com/drive/folders/1ArGpcI2-80m3zkJGAKsoPBJ2fstpveBQ?usp=drive_link  
Reflection and summary:  
In this version, I have adjusted the scale of the content image to a few. From the final output picture, you can see the "orange slice" in the content picture more completely. In order to make the output image more interesting, I adjusted the texture value to make the output image more textured. I think this version is also a good attempt. The color is also relatively bright and bright. It's a good try.  
